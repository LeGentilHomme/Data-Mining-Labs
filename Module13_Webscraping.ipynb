{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Module13_Webscraping.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOHCkCEA2qX+Gm6OSI9EOc7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bC4Y-s9M46C4"},"source":["# **Module 13: Webscraping**\n","\n","Up to this point, we have mainly learned how the most popular data analytics algorithms work, how to preprocess our data to get them to work, and how to configure these algorithms to get them to work faster and better. For that, we always had \"canned\" data available, meaning datasets that were already in some sort of .csv format, nicely tabulated, and ready for analysis.\n","\n","But real life is harsh. Data doesn't usually show up in neat little (or big) .csv packaging. It is messy, crazy, and unstructured. Think, for example, about product ratings on Amazon.com, comments on YouTube or Instagram, or threads of tweets on Twitter, video responses to other videos on TikTok, likes/ dislikes and donations on Discord or Twitch, and so on. There's a whole lot of data there, and it can be incredibly useful. But 1. how do you get to it and 2. how do you analyze it? That's what this module is all about.\n","\n","At the end of this module, you will be able to:\n","* Acquire data from a webpage\n","* Clean data obtained from a webpage\n","* Acquire data from an API\n","\n","Let's go."]},{"cell_type":"markdown","metadata":{"id":"eCgICS7T7HcW"},"source":["# **0. Preparation and Setup**\n","Well, we need our libraries again, this time for webscraping and for textual analysis, which we will do in the second half of this file."]},{"cell_type":"code","metadata":{"id":"D1HiXStc80Nq"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6JzaiLW_8e31"},"source":["# **1. Web Scraping**\n","Web scraping is the process of using code to extract content and data from a website. It's kind of like building your own web search and using it to scour the internet for appropriate data--for example, sites like LinkedIn or Indeed for the title of the job you want after you graduate, or sites like [catster.com](https://www.catster.com/), Youtube.com or Reddit's [CatAdvice subreddit](https://www.reddit.com/r/CatAdvice/) for, say, information about healthy cat food.\n","\n","To perform web scraping, we will import the libraries shown below. The [urllib.request](https://docs.python.org/3/library/urllib.request.html) module is used to open URLs. The [Beautiful Soup package](https://pypi.org/project/beautifulsoup4/) is used to extract data from html files. The Beautiful Soup library's name is bs4 which stands for Beautiful Soup, version 4.\n","\n","This is an amended copy of the [Datacamp Tutorial on Web Scraping](https://www.datacamp.com/community/tutorials/web-scraping-using-python)."]},{"cell_type":"code","metadata":{"id":"Sx1lPPqa42qM"},"source":["from urllib.request import urlopen\n","from bs4 import BeautifulSoup"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RFgMgvNYIYzi"},"source":["Now we specify the URL containing the dataset and pass it to urlopen() to get the html of the page.\n","\n","**NOTE** that some pages will produce the following error: `HTTPError: HTTP Error 403: Forbidden`; the developers built in anti-scraping security code."]},{"cell_type":"code","metadata":{"id":"3VJ93kJAJSKm"},"source":["url = \"https://www.hubertiming.com/results/2017GPTR10K\"\n","# url = \"http://help.websiteos.com/websiteos/example_of_a_simple_html_page.htm\"\n","# url = \"https://www.zyte.com/learn/what-is-web-scraping/\"\n","html = urlopen(url)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M358QAA7IbxT"},"source":["Getting the html of the page is just the first step. Next step is to create a Beautiful Soup object from the html. This is done by passing the html to the BeautifulSoup() function. The Beautiful Soup package is used to parse the html, that is, take the raw html text and break it into Python objects. The second argument 'lxml' is the html parser which assigns the Python objects to the appropriate tag delimiters."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GK5n3qFoIvqJ","executionInfo":{"status":"ok","timestamp":1627300519208,"user_tz":300,"elapsed":378,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"69d49861-dcfc-4d90-ce9e-e39d70a9e590"},"source":["soup = BeautifulSoup(html, 'lxml')\n","type(soup)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["bs4.BeautifulSoup"]},"metadata":{"tags":[]},"execution_count":78}]},{"cell_type":"markdown","metadata":{"id":"XfReP-hYIsiN"},"source":["Now we use the soup object to extract interesting information about the website we are scraping such as getting the title of the page as shown below."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0hEq6StTKUS9","executionInfo":{"status":"ok","timestamp":1627300522661,"user_tz":300,"elapsed":227,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"b73c1b1f-05b8-4773-9d98-1a7336d0bbf9"},"source":["# Get the title\n","title = soup.title\n","print(title)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<title>Race results for the 2017 Intel Great Place to Run \\ Urban Clash Games!</title>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o94wny6mJzkA"},"source":["You can also get the text of the webpage and quickly print it out to check if it is what you expect."]},{"cell_type":"code","metadata":{"id":"WU_0XKRvLabT"},"source":["# Print out the text\n","text = soup.get_text()\n","print(soup.text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8hvR42SKJ4Tr"},"source":["Now, open a new tab on your web browser and go directly to the website you are scraping. Right-click into the website and, from the popup menu, select \"Inspect.\" If you are in Chrome, this will open a developer view with many tabs on the right side of your screen. This will show you the code of the webpage (although you may have to open and close a number of expanders to see any actual HTML tags).\n","\n","<div>\n","<center>\n","<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/webscraping_HTML_example.png\" width=\"500\">\n","</div>\n","\n","You can use the find_all() method of soup to extract useful html tags within a webpage. Examples of useful tags include < a > for hyperlinks, < table > for tables, < tr > for table rows, < th > for table headers, and < td > for table cells. The code below shows how to extract all the hyperlinks within the webpage.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H6no8COcLwBZ","executionInfo":{"status":"ok","timestamp":1627300574333,"user_tz":300,"elapsed":203,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"cc042575-b05e-418c-aa18-929bd8fd716a"},"source":["soup.find_all('a')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<a href=\"mailto:timing@hubertiming.com\">timing@hubertiming.com</a>,\n"," <a href=\"https://www.hubertiming.com/\">Huber Timing Home</a>,\n"," <a class=\"btn btn-primary btn-lg\" href=\"/results/2017GPTR#individual\" role=\"button\" style=\"margin: 0px 0px 5px 5px\"><i aria-hidden=\"true\" class=\"fa fa-user\"></i> 5K Individual</a>,\n"," <a class=\"btn btn-primary btn-lg\" href=\"/results/team/2017GPTR\" role=\"button\" style=\"margin: 0px 0px 5px 5px\"><i aria-hidden=\"true\" class=\"fa fa-users\"></i> 5K Team</a>,\n"," <a class=\"btn btn-primary btn-lg\" href=\"/results/team/2017GPTR10K\" role=\"button\" style=\"margin: 0px 0px 5px 5px\"><i aria-hidden=\"true\" class=\"fa fa-users\"></i> 10K Team</a>,\n"," <a class=\"btn btn-primary btn-lg\" href=\"/results/summary/2017GPTR10K\" role=\"button\" style=\"margin: 0px 0px 5px 5px\"><i class=\"fa fa-stream\"></i> Summary</a>,\n"," <a id=\"individual\" name=\"individual\"></a>,\n"," <a data-url=\"/results/2017GPTR10K\" href=\"#tabs-1\" id=\"rootTab\" style=\"font-size: 18px\">10K Results</a>,\n"," <a href=\"https://www.hubertiming.com/\"><img height=\"65\" src=\"https://www.hubertiming.com//sites/all/themes/hubertiming/images/clockWithFinishSign_small.png\" width=\"50\"/>Huber Timing</a>,\n"," <a href=\"https://facebook.com/hubertiming/\"><img src=\"https://www.hubertiming.com/results/FB-f-Logo__blue_50.png\"/></a>,\n"," <a class=\"small\" id=\"bestFeatureEver\" style=\"color:#007bff\">Dark Mode</a>]"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"markdown","metadata":{"id":"-jry1kykpJKJ"},"source":["As you can see from the output above, html tags sometimes come with attributes such as class, src, etc. These attributes provide additional information about html elements. You can use a for loop and the get('\"href\") method to extract and print out only hyperlinks."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3_8fuxBpM8q","executionInfo":{"status":"ok","timestamp":1627300581283,"user_tz":300,"elapsed":226,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"54319d6b-2b0a-4ddc-eee5-3521896aa627"},"source":["all_links = soup.find_all(\"a\")\n","for link in all_links:\n","    print(link.get(\"href\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mailto:timing@hubertiming.com\n","https://www.hubertiming.com/\n","/results/2017GPTR#individual\n","/results/team/2017GPTR\n","/results/team/2017GPTR10K\n","/results/summary/2017GPTR10K\n","None\n","#tabs-1\n","https://www.hubertiming.com/\n","https://facebook.com/hubertiming/\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nKM62MaTpP2P"},"source":["To print out table rows only, pass the 'tr' argument in soup.find_all()."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x4A3ATPdpTOA","executionInfo":{"status":"ok","timestamp":1627301364574,"user_tz":300,"elapsed":209,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"763b12d7-b03b-44ec-e628-06f5b7022af6"},"source":["# Print the first 7 rows for sanity check\n","rows = soup.find_all('tr')\n","print(rows [:7])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[<tr colspan=\"2\">\n","<b>10K:</b>\n","</tr>, <tr>\n","<td>Finishers:</td>\n","<td>577</td>\n","</tr>, <tr>\n","<td>Male:</td>\n","<td>414</td>\n","</tr>, <tr>\n","<td>Female:</td>\n","<td>163</td>\n","</tr>, <tr class=\"header\">\n","<th>Place</th>\n","<th>Bib</th>\n","<th>Name</th>\n","<th>Gender</th>\n","<th>City</th>\n","<th>State</th>\n","<th>Chip Time</th>\n","<th>Chip Pace</th>\n","<th>Gun Time</th>\n","<th>Team</th>\n","</tr>, <tr data-bib=\"814\">\n","<td>1</td>\n","<td>814</td>\n","<td>\r\n","\r\n","                    JARED WILSON\r\n","\r\n","                </td>\n","<td>M</td>\n","<td>TIGARD</td>\n","<td>OR</td>\n","<td>36:21</td>\n","<td>5:51</td>\n","<td>36:24</td>\n","<td></td>\n","</tr>, <tr data-bib=\"573\">\n","<td>2</td>\n","<td>573</td>\n","<td>\r\n","\r\n","                    NATHAN A SUSTERSIC\r\n","\r\n","                </td>\n","<td>M</td>\n","<td>PORTLAND</td>\n","<td>OR</td>\n","<td>36:42</td>\n","<td>5:55</td>\n","<td>36:45</td>\n","<td>\n","<img class=\"lazy teamThumbs\" data-src=\"/teamLogoThumbnail/logo?teamName=INTEL%20TEAM%20F&amp;raceId=1251&amp;state=OR\"/>\r\n","                            INTEL TEAM F\r\n","                        </td>\n","</tr>]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aq4LWlIPsQ_9"},"source":["## **1.1. Preprocessing**\n","Our goal here is to convert the data from the webpage into a dataframe so we can do our data magic with it. To get there, we need to get all table rows in list form first and then convert that list into a dataframe. Below is a for loop that iterates through table rows and prints out the cells of the rows."]},{"cell_type":"markdown","metadata":{"id":"-CbnHKJGs3gi"},"source":["### **1.1.1 Extracting data from table rows**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WmXVPSC6scpN","executionInfo":{"status":"ok","timestamp":1627301502280,"user_tz":300,"elapsed":208,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"684c8e3a-0ba8-4c05-9df9-6b7c5c259e9e"},"source":["for row in rows:\n","    row_td = row.find_all('td')\n","print(row_td)\n","type(row_td)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[<td>577</td>, <td>443</td>, <td>\r\n","\r\n","                    LIBBY B MITCHELL\r\n","\r\n","                </td>, <td>F</td>, <td>HILLSBORO</td>, <td>OR</td>, <td>1:41:18</td>, <td>16:20</td>, <td>1:42:10</td>, <td></td>]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["bs4.element.ResultSet"]},"metadata":{"tags":[]},"execution_count":95}]},{"cell_type":"markdown","metadata":{"id":"UMFxYdpQrMO7"},"source":["### **1.1.2. Cleaning Data: Removing HTML Tags**\n","\n","The output above shows that each row is printed with html tags embedded in each row. This is not what you want. You can use remove the html tags using Beautiful Soup or regular expressions.\n","\n","The easiest way to remove html tags is to use Beautiful Soup, and it takes just one line of code to do this. Pass the string of interest into BeautifulSoup() and use the get_text() method to extract the text without html tags."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZcVGdEBwrN3E","executionInfo":{"status":"ok","timestamp":1627302143100,"user_tz":300,"elapsed":377,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"1c9169d5-026c-4d83-ca9b-4a435e8d3bcd"},"source":["str_cells = str(row_td)\n","cleantext = BeautifulSoup(str_cells, \"lxml\").get_text()\n","print(cleantext)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[577, 443, \r\n","\r\n","                    LIBBY B MITCHELL\r\n","\r\n","                , F, HILLSBORO, OR, 1:41:18, 16:20, 1:42:10, ]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aG-VZpuPuqjY"},"source":["The code below shows how to build a regular expression that finds all the characters inside the < td > html tags and replace them with an empty string for each table row. First, you compile a regular expression by passing a string to match to re.compile(). The dot, star, and question mark (.*?) will match an opening angle bracket followed by anything and followed by a closing angle bracket. It matches text in a non-greedy fashion, that is, it matches the shortest possible string. If you omit the question mark, it will match all the text between the first opening angle bracket and the last closing angle bracket. After compiling a regular expression, you can use the re.sub() method to find all the substrings where the regular expression matches and replace them with an empty string. The full code below generates an empty list, extract text in between html tags for each row, and append it to the assigned list."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JwqiJPuxsOhj","executionInfo":{"status":"ok","timestamp":1627302156214,"user_tz":300,"elapsed":265,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"c5d5f89c-d84a-427c-cfb7-1157111f9b9b"},"source":["import re\n","\n","list_rows = []\n","for row in rows:\n","    cells = row.find_all('td')\n","    str_cells = str(cells)\n","    clean = re.compile('<.*?>')  # matches an opening angle bracket followed by anything and followed by a closing angle bracket\n","    clean2 = (re.sub(clean, '',str_cells))\n","    list_rows.append(clean2)\n","print(clean2)\n","type(clean2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[577, 443, \r\n","\r\n","                    LIBBY B MITCHELL\r\n","\r\n","                , F, HILLSBORO, OR, 1:41:18, 16:20, 1:42:10, ]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["str"]},"metadata":{"tags":[]},"execution_count":109}]},{"cell_type":"markdown","metadata":{"id":"J-_-tFt6u2pZ"},"source":["## **1.2 Converting Data to Dataframe**\n","The next step is to convert the list into a dataframe and get a quick view of the first 10 rows using Pandas."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"id":"8GQybEfxu5tl","executionInfo":{"status":"ok","timestamp":1627302310470,"user_tz":300,"elapsed":234,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"a29b55ad-586f-4ab7-fe3d-c7082b40c49a"},"source":["df = pd.DataFrame(list_rows)\n","df.head(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[Finishers:, 577]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[Male:, 414]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[Female:, 163]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>[1, 814, \\r\\n\\r\\n                    JARED WIL...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>[2, 573, \\r\\n\\r\\n                    NATHAN A ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>[3, 687, \\r\\n\\r\\n                    FRANCISCO...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>[4, 623, \\r\\n\\r\\n                    PAUL MORR...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>[5, 569, \\r\\n\\r\\n                    DEREK G O...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                   0\n","0                                                 []\n","1                                  [Finishers:, 577]\n","2                                       [Male:, 414]\n","3                                     [Female:, 163]\n","4                                                 []\n","5  [1, 814, \\r\\n\\r\\n                    JARED WIL...\n","6  [2, 573, \\r\\n\\r\\n                    NATHAN A ...\n","7  [3, 687, \\r\\n\\r\\n                    FRANCISCO...\n","8  [4, 623, \\r\\n\\r\\n                    PAUL MORR...\n","9  [5, 569, \\r\\n\\r\\n                    DEREK G O..."]},"metadata":{"tags":[]},"execution_count":113}]},{"cell_type":"markdown","metadata":{"id":"sK8vdjEfvQxm"},"source":["### **1.2.1 Cleaning Data: Formatting the Dataframe**\n","The dataframe is not in the format we want. To clean it up, you should split the \"0\" column into multiple columns at the comma position. This is accomplished by using the str.split() method."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"id":"mRxojNgPvjUo","executionInfo":{"status":"ok","timestamp":1627301764842,"user_tz":300,"elapsed":214,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"54d832f3-3301-4341-c37c-41c427a962e1"},"source":["df1 = df[0].str.split(',', expand=True)\n","df1.head(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[Finishers:</td>\n","      <td>577]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[Male:</td>\n","      <td>414]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[Female:</td>\n","      <td>163]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>[1</td>\n","      <td>814</td>\n","      <td>\\r\\n\\r\\n                    JARED WILSON\\r\\n\\...</td>\n","      <td>M</td>\n","      <td>TIGARD</td>\n","      <td>OR</td>\n","      <td>36:21</td>\n","      <td>5:51</td>\n","      <td>36:24</td>\n","      <td>]</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>[2</td>\n","      <td>573</td>\n","      <td>\\r\\n\\r\\n                    NATHAN A SUSTERSI...</td>\n","      <td>M</td>\n","      <td>PORTLAND</td>\n","      <td>OR</td>\n","      <td>36:42</td>\n","      <td>5:55</td>\n","      <td>36:45</td>\n","      <td>\\n\\r\\n                            INTEL TEAM ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>[3</td>\n","      <td>687</td>\n","      <td>\\r\\n\\r\\n                    FRANCISCO MAYA\\r\\...</td>\n","      <td>M</td>\n","      <td>PORTLAND</td>\n","      <td>OR</td>\n","      <td>37:44</td>\n","      <td>6:05</td>\n","      <td>37:48</td>\n","      <td>]</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>[4</td>\n","      <td>623</td>\n","      <td>\\r\\n\\r\\n                    PAUL MORROW\\r\\n\\r...</td>\n","      <td>M</td>\n","      <td>BEAVERTON</td>\n","      <td>OR</td>\n","      <td>38:34</td>\n","      <td>6:13</td>\n","      <td>38:37</td>\n","      <td>]</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>[5</td>\n","      <td>569</td>\n","      <td>\\r\\n\\r\\n                    DEREK G OSBORNE\\r...</td>\n","      <td>M</td>\n","      <td>HILLSBORO</td>\n","      <td>OR</td>\n","      <td>39:21</td>\n","      <td>6:20</td>\n","      <td>39:24</td>\n","      <td>\\n\\r\\n                            INTEL TEAM ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             0  ...                                                  9\n","0           []  ...                                               None\n","1  [Finishers:  ...                                               None\n","2       [Male:  ...                                               None\n","3     [Female:  ...                                               None\n","4           []  ...                                               None\n","5           [1  ...                                                  ]\n","6           [2  ...   \\n\\r\\n                            INTEL TEAM ...\n","7           [3  ...                                                  ]\n","8           [4  ...                                                  ]\n","9           [5  ...   \\n\\r\\n                            INTEL TEAM ...\n","\n","[10 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":98}]},{"cell_type":"markdown","metadata":{"id":"thF_lW95vssg"},"source":["This looks much better, but there is still work to do. The dataframe has unwanted square brackets surrounding each row. You can use the strip() method to remove the opening square bracket on column \"0.\""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"id":"1ORmweKyvrjv","executionInfo":{"status":"ok","timestamp":1627303144029,"user_tz":300,"elapsed":232,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"794f60d3-6116-4970-9735-23316d4a2d7a"},"source":["df1[0] = df1[0].str.strip('[')\n","df1.head(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Finishers:</td>\n","      <td>577]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Male:</td>\n","      <td>414]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Female:</td>\n","      <td>163]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>814</td>\n","      <td>\\r\\n\\r\\n                    JARED WILSON\\r\\n\\...</td>\n","      <td>M</td>\n","      <td>TIGARD</td>\n","      <td>OR</td>\n","      <td>36:21</td>\n","      <td>5:51</td>\n","      <td>36:24</td>\n","      <td>]</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2</td>\n","      <td>573</td>\n","      <td>\\r\\n\\r\\n                    NATHAN A SUSTERSI...</td>\n","      <td>M</td>\n","      <td>PORTLAND</td>\n","      <td>OR</td>\n","      <td>36:42</td>\n","      <td>5:55</td>\n","      <td>36:45</td>\n","      <td>\\n\\r\\n                            INTEL TEAM ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>3</td>\n","      <td>687</td>\n","      <td>\\r\\n\\r\\n                    FRANCISCO MAYA\\r\\...</td>\n","      <td>M</td>\n","      <td>PORTLAND</td>\n","      <td>OR</td>\n","      <td>37:44</td>\n","      <td>6:05</td>\n","      <td>37:48</td>\n","      <td>]</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>4</td>\n","      <td>623</td>\n","      <td>\\r\\n\\r\\n                    PAUL MORROW\\r\\n\\r...</td>\n","      <td>M</td>\n","      <td>BEAVERTON</td>\n","      <td>OR</td>\n","      <td>38:34</td>\n","      <td>6:13</td>\n","      <td>38:37</td>\n","      <td>]</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>5</td>\n","      <td>569</td>\n","      <td>\\r\\n\\r\\n                    DEREK G OSBORNE\\r...</td>\n","      <td>M</td>\n","      <td>HILLSBORO</td>\n","      <td>OR</td>\n","      <td>39:21</td>\n","      <td>6:20</td>\n","      <td>39:24</td>\n","      <td>\\n\\r\\n                            INTEL TEAM ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            0      1  ...       8                                                  9\n","0           ]   None  ...    None                                               None\n","1  Finishers:   577]  ...    None                                               None\n","2       Male:   414]  ...    None                                               None\n","3     Female:   163]  ...    None                                               None\n","4           ]   None  ...    None                                               None\n","5           1    814  ...   36:24                                                  ]\n","6           2    573  ...   36:45   \\n\\r\\n                            INTEL TEAM ...\n","7           3    687  ...   37:48                                                  ]\n","8           4    623  ...   38:37                                                  ]\n","9           5    569  ...   39:24   \\n\\r\\n                            INTEL TEAM ...\n","\n","[10 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":122}]},{"cell_type":"markdown","metadata":{"id":"XWvlypIU0JBG"},"source":["### **1.2.2 Building Table Headers**"]},{"cell_type":"markdown","metadata":{"id":"7fE8lvxcv60s"},"source":["The table is missing table headers. You can use the find_all() method to get the table headers."]},{"cell_type":"code","metadata":{"id":"5VGxmrqTv8N9"},"source":["col_labels = soup.find_all('th')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A5Db-oXXv_c-"},"source":["Just like what we did with the table rows, you can use Beautiful Soup to extract text in between html tags for table headers."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RLJjigxtwETr","executionInfo":{"status":"ok","timestamp":1627301901167,"user_tz":300,"elapsed":272,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"339cc509-0a2a-4e0e-f25d-d7ae13b59563"},"source":["all_header = []\n","col_str = str(col_labels)\n","cleantext2 = BeautifulSoup(col_str, \"lxml\").get_text()\n","all_header.append(cleantext2)\n","print(all_header)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['[Place, Bib, Name, Gender, City, State, Chip Time, Chip Pace, Gun Time, Team]']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0ECOElNswNWH"},"source":["You can then convert the list of headers into a pandas dataframe."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":79},"id":"C67QY3eYwO8L","executionInfo":{"status":"ok","timestamp":1627301944955,"user_tz":300,"elapsed":462,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"edc094f2-4622-43db-caf1-bc2144fc2122"},"source":["df2 = pd.DataFrame(all_header)\n","df2.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[Place, Bib, Name, Gender, City, State, Chip T...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                   0\n","0  [Place, Bib, Name, Gender, City, State, Chip T..."]},"metadata":{"tags":[]},"execution_count":102}]},{"cell_type":"markdown","metadata":{"id":"EyVutGzvwXpC"},"source":["Similarly, you can split column \"0\" into multiple columns at the comma position for all rows."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":79},"id":"yeX95KzIwTon","executionInfo":{"status":"ok","timestamp":1627302000155,"user_tz":300,"elapsed":395,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"d8844581-54a9-4923-ce84-281a3752ca2d"},"source":["df3 = df2[0].str.split(',', expand=True)\n","df3.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[Place</td>\n","      <td>Bib</td>\n","      <td>Name</td>\n","      <td>Gender</td>\n","      <td>City</td>\n","      <td>State</td>\n","      <td>Chip Time</td>\n","      <td>Chip Pace</td>\n","      <td>Gun Time</td>\n","      <td>Team]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        0     1      2        3  ...           6           7          8       9\n","0  [Place   Bib   Name   Gender  ...   Chip Time   Chip Pace   Gun Time   Team]\n","\n","[1 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":103}]},{"cell_type":"markdown","metadata":{"id":"h5dI_GfZwjEh"},"source":["Now we can concatenate the two dataframes into one using the concat() method as illustrated below."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"id":"jKOHMzHAwoHk","executionInfo":{"status":"ok","timestamp":1627302047629,"user_tz":300,"elapsed":268,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"97538a9e-0755-4813-a6f2-4d45a3f491ae"},"source":["frames = [df3, df1]\n","\n","df4 = pd.concat(frames)\n","df4.head(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[Place</td>\n","      <td>Bib</td>\n","      <td>Name</td>\n","      <td>Gender</td>\n","      <td>City</td>\n","      <td>State</td>\n","      <td>Chip Time</td>\n","      <td>Chip Pace</td>\n","      <td>Gun Time</td>\n","      <td>Team]</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Finishers:</td>\n","      <td>577]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Male:</td>\n","      <td>414]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Female:</td>\n","      <td>163]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>814</td>\n","      <td>\\r\\n\\r\\n                    JARED WILSON\\r\\n\\...</td>\n","      <td>M</td>\n","      <td>TIGARD</td>\n","      <td>OR</td>\n","      <td>36:21</td>\n","      <td>5:51</td>\n","      <td>36:24</td>\n","      <td>]</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2</td>\n","      <td>573</td>\n","      <td>\\r\\n\\r\\n                    NATHAN A SUSTERSI...</td>\n","      <td>M</td>\n","      <td>PORTLAND</td>\n","      <td>OR</td>\n","      <td>36:42</td>\n","      <td>5:55</td>\n","      <td>36:45</td>\n","      <td>\\n\\r\\n                            INTEL TEAM ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>3</td>\n","      <td>687</td>\n","      <td>\\r\\n\\r\\n                    FRANCISCO MAYA\\r\\...</td>\n","      <td>M</td>\n","      <td>PORTLAND</td>\n","      <td>OR</td>\n","      <td>37:44</td>\n","      <td>6:05</td>\n","      <td>37:48</td>\n","      <td>]</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>4</td>\n","      <td>623</td>\n","      <td>\\r\\n\\r\\n                    PAUL MORROW\\r\\n\\r...</td>\n","      <td>M</td>\n","      <td>BEAVERTON</td>\n","      <td>OR</td>\n","      <td>38:34</td>\n","      <td>6:13</td>\n","      <td>38:37</td>\n","      <td>]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            0  ...                                                  9\n","0      [Place  ...                                              Team]\n","0           ]  ...                                               None\n","1  Finishers:  ...                                               None\n","2       Male:  ...                                               None\n","3     Female:  ...                                               None\n","4           ]  ...                                               None\n","5           1  ...                                                  ]\n","6           2  ...   \\n\\r\\n                            INTEL TEAM ...\n","7           3  ...                                                  ]\n","8           4  ...                                                  ]\n","\n","[10 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":104}]},{"cell_type":"markdown","metadata":{"id":"iIiZxff4wvOb"},"source":["Below shows how to assign the first row to be the table header."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"emJ62qOQwwRs","executionInfo":{"status":"ok","timestamp":1627302079361,"user_tz":300,"elapsed":335,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"c22fe07b-7c61-480d-e5ce-9445e3b2e550"},"source":["df5 = df4.rename(columns=df4.iloc[0])\n","df5.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>[Place</th>\n","      <th>Bib</th>\n","      <th>Name</th>\n","      <th>Gender</th>\n","      <th>City</th>\n","      <th>State</th>\n","      <th>Chip Time</th>\n","      <th>Chip Pace</th>\n","      <th>Gun Time</th>\n","      <th>Team]</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[Place</td>\n","      <td>Bib</td>\n","      <td>Name</td>\n","      <td>Gender</td>\n","      <td>City</td>\n","      <td>State</td>\n","      <td>Chip Time</td>\n","      <td>Chip Pace</td>\n","      <td>Gun Time</td>\n","      <td>Team]</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Finishers:</td>\n","      <td>577]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Male:</td>\n","      <td>414]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Female:</td>\n","      <td>163]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       [Place    Bib   Name   Gender  ...   Chip Time   Chip Pace   Gun Time   Team]\n","0      [Place    Bib   Name   Gender  ...   Chip Time   Chip Pace   Gun Time   Team]\n","0           ]   None   None     None  ...        None        None       None    None\n","1  Finishers:   577]   None     None  ...        None        None       None    None\n","2       Male:   414]   None     None  ...        None        None       None    None\n","3     Female:   163]   None     None  ...        None        None       None    None\n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":105}]},{"cell_type":"markdown","metadata":{"id":"NHajiGAZw2yW"},"source":["At this point, the table is almost properly formatted. For analysis, you can start by getting an overview of the data as shown below."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IDuqrSGHw4FC","executionInfo":{"status":"ok","timestamp":1627302112892,"user_tz":300,"elapsed":258,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"4d4db98d-3de4-4446-a341-0dca55bac4f5"},"source":["df5.info()\n","df5.shape"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 583 entries, 0 to 581\n","Data columns (total 10 columns):\n"," #   Column      Non-Null Count  Dtype \n","---  ------      --------------  ----- \n"," 0   [Place      583 non-null    object\n"," 1    Bib        581 non-null    object\n"," 2    Name       578 non-null    object\n"," 3    Gender     578 non-null    object\n"," 4    City       578 non-null    object\n"," 5    State      578 non-null    object\n"," 6    Chip Time  578 non-null    object\n"," 7    Chip Pace  578 non-null    object\n"," 8    Gun Time   578 non-null    object\n"," 9    Team]      578 non-null    object\n","dtypes: object(10)\n","memory usage: 50.1+ KB\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(583, 10)"]},"metadata":{"tags":[]},"execution_count":106}]},{"cell_type":"markdown","metadata":{"id":"TbvKS3Aov6lH"},"source":["The table has 583 rows and 10 columns. You can drop all rows with any missing values."]},{"cell_type":"code","metadata":{"id":"YUyoUt9FyQQo"},"source":["df6 = df5.dropna(axis=0, how='any')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y0OVuq8WyW19"},"source":["Also, notice how the table header is replicated as the first row in df5. It can be dropped using the following line of code."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"mCtAiQhvyYLV","executionInfo":{"status":"ok","timestamp":1627303867642,"user_tz":300,"elapsed":331,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"f079938a-ce1d-40bc-acf5-1ecaf6b5a1c8"},"source":["df7 = df6.drop(df6.index[0])\n","df7.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>[Place</th>\n","      <th>Bib</th>\n","      <th>Name</th>\n","      <th>Gender</th>\n","      <th>City</th>\n","      <th>State</th>\n","      <th>Chip Time</th>\n","      <th>Chip Pace</th>\n","      <th>Gun Time</th>\n","      <th>Team]</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>814</td>\n","      <td>\\r\\n\\r\\n                    JARED WILSON\\r\\n\\...</td>\n","      <td>M</td>\n","      <td>TIGARD</td>\n","      <td>OR</td>\n","      <td>36:21</td>\n","      <td>5:51</td>\n","      <td>36:24</td>\n","      <td>]</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2</td>\n","      <td>573</td>\n","      <td>\\r\\n\\r\\n                    NATHAN A SUSTERSI...</td>\n","      <td>M</td>\n","      <td>PORTLAND</td>\n","      <td>OR</td>\n","      <td>36:42</td>\n","      <td>5:55</td>\n","      <td>36:45</td>\n","      <td>\\n\\r\\n                            INTEL TEAM ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>3</td>\n","      <td>687</td>\n","      <td>\\r\\n\\r\\n                    FRANCISCO MAYA\\r\\...</td>\n","      <td>M</td>\n","      <td>PORTLAND</td>\n","      <td>OR</td>\n","      <td>37:44</td>\n","      <td>6:05</td>\n","      <td>37:48</td>\n","      <td>]</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>4</td>\n","      <td>623</td>\n","      <td>\\r\\n\\r\\n                    PAUL MORROW\\r\\n\\r...</td>\n","      <td>M</td>\n","      <td>BEAVERTON</td>\n","      <td>OR</td>\n","      <td>38:34</td>\n","      <td>6:13</td>\n","      <td>38:37</td>\n","      <td>]</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>5</td>\n","      <td>569</td>\n","      <td>\\r\\n\\r\\n                    DEREK G OSBORNE\\r...</td>\n","      <td>M</td>\n","      <td>HILLSBORO</td>\n","      <td>OR</td>\n","      <td>39:21</td>\n","      <td>6:20</td>\n","      <td>39:24</td>\n","      <td>\\n\\r\\n                            INTEL TEAM ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  [Place   Bib  ...  Gun Time                                              Team]\n","5      1   814  ...     36:24                                                  ]\n","6      2   573  ...     36:45   \\n\\r\\n                            INTEL TEAM ...\n","7      3   687  ...     37:48                                                  ]\n","8      4   623  ...     38:37                                                  ]\n","9      5   569  ...     39:24   \\n\\r\\n                            INTEL TEAM ...\n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":139}]},{"cell_type":"markdown","metadata":{"id":"l-7_KfFwyl18"},"source":["You can perform more data cleaning by renaming the '[Place' and ' Team]' columns. Python is very picky about space. Make sure you include space after the quotation mark in ' Team]'."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"ncRmXhG0ym9S","executionInfo":{"status":"ok","timestamp":1627303884392,"user_tz":300,"elapsed":258,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"80c29134-afa9-4372-c8ce-fa36aac05126"},"source":["df7.rename(columns={'[Place': 'Place'},inplace=True)\n","df7.rename(columns={' Team]': 'Team'},inplace=True)\n","df7.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Place</th>\n","      <th>Bib</th>\n","      <th>Name</th>\n","      <th>Gender</th>\n","      <th>City</th>\n","      <th>State</th>\n","      <th>Chip Time</th>\n","      <th>Chip Pace</th>\n","      <th>Gun Time</th>\n","      <th>Team</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>814</td>\n","      <td>\\r\\n\\r\\n                    JARED WILSON\\r\\n\\...</td>\n","      <td>M</td>\n","      <td>TIGARD</td>\n","      <td>OR</td>\n","      <td>36:21</td>\n","      <td>5:51</td>\n","      <td>36:24</td>\n","      <td>]</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2</td>\n","      <td>573</td>\n","      <td>\\r\\n\\r\\n                    NATHAN A SUSTERSI...</td>\n","      <td>M</td>\n","      <td>PORTLAND</td>\n","      <td>OR</td>\n","      <td>36:42</td>\n","      <td>5:55</td>\n","      <td>36:45</td>\n","      <td>\\n\\r\\n                            INTEL TEAM ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>3</td>\n","      <td>687</td>\n","      <td>\\r\\n\\r\\n                    FRANCISCO MAYA\\r\\...</td>\n","      <td>M</td>\n","      <td>PORTLAND</td>\n","      <td>OR</td>\n","      <td>37:44</td>\n","      <td>6:05</td>\n","      <td>37:48</td>\n","      <td>]</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>4</td>\n","      <td>623</td>\n","      <td>\\r\\n\\r\\n                    PAUL MORROW\\r\\n\\r...</td>\n","      <td>M</td>\n","      <td>BEAVERTON</td>\n","      <td>OR</td>\n","      <td>38:34</td>\n","      <td>6:13</td>\n","      <td>38:37</td>\n","      <td>]</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>5</td>\n","      <td>569</td>\n","      <td>\\r\\n\\r\\n                    DEREK G OSBORNE\\r...</td>\n","      <td>M</td>\n","      <td>HILLSBORO</td>\n","      <td>OR</td>\n","      <td>39:21</td>\n","      <td>6:20</td>\n","      <td>39:24</td>\n","      <td>\\n\\r\\n                            INTEL TEAM ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Place   Bib  ...  Gun Time                                               Team\n","5     1   814  ...     36:24                                                  ]\n","6     2   573  ...     36:45   \\n\\r\\n                            INTEL TEAM ...\n","7     3   687  ...     37:48                                                  ]\n","8     4   623  ...     38:37                                                  ]\n","9     5   569  ...     39:24   \\n\\r\\n                            INTEL TEAM ...\n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":140}]},{"cell_type":"markdown","metadata":{"id":"vw7noXMvywV4"},"source":["## **1.3 Data Cleaning: Removing White Space and Special Characters**\n","\n","The final data cleaning steps involve removing the closing bracket for cells in the \"Team\" column, the white space, and the new line characters"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"Ix2-TqEPyx3i","executionInfo":{"status":"ok","timestamp":1627303895425,"user_tz":300,"elapsed":342,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"2ebcc581-3452-429a-a35e-0383ab4e7366"},"source":["df7['Team'] = df7['Team'].str.strip(']')\n","df7.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Place</th>\n","      <th>Bib</th>\n","      <th>Name</th>\n","      <th>Gender</th>\n","      <th>City</th>\n","      <th>State</th>\n","      <th>Chip Time</th>\n","      <th>Chip Pace</th>\n","      <th>Gun Time</th>\n","      <th>Team</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>814</td>\n","      <td>\\r\\n\\r\\n                    JARED WILSON\\r\\n\\...</td>\n","      <td>M</td>\n","      <td>TIGARD</td>\n","      <td>OR</td>\n","      <td>36:21</td>\n","      <td>5:51</td>\n","      <td>36:24</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2</td>\n","      <td>573</td>\n","      <td>\\r\\n\\r\\n                    NATHAN A SUSTERSI...</td>\n","      <td>M</td>\n","      <td>PORTLAND</td>\n","      <td>OR</td>\n","      <td>36:42</td>\n","      <td>5:55</td>\n","      <td>36:45</td>\n","      <td>\\n\\r\\n                            INTEL TEAM ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>3</td>\n","      <td>687</td>\n","      <td>\\r\\n\\r\\n                    FRANCISCO MAYA\\r\\...</td>\n","      <td>M</td>\n","      <td>PORTLAND</td>\n","      <td>OR</td>\n","      <td>37:44</td>\n","      <td>6:05</td>\n","      <td>37:48</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>4</td>\n","      <td>623</td>\n","      <td>\\r\\n\\r\\n                    PAUL MORROW\\r\\n\\r...</td>\n","      <td>M</td>\n","      <td>BEAVERTON</td>\n","      <td>OR</td>\n","      <td>38:34</td>\n","      <td>6:13</td>\n","      <td>38:37</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>5</td>\n","      <td>569</td>\n","      <td>\\r\\n\\r\\n                    DEREK G OSBORNE\\r...</td>\n","      <td>M</td>\n","      <td>HILLSBORO</td>\n","      <td>OR</td>\n","      <td>39:21</td>\n","      <td>6:20</td>\n","      <td>39:24</td>\n","      <td>\\n\\r\\n                            INTEL TEAM ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Place   Bib  ...  Gun Time                                               Team\n","5     1   814  ...     36:24                                                   \n","6     2   573  ...     36:45   \\n\\r\\n                            INTEL TEAM ...\n","7     3   687  ...     37:48                                                   \n","8     4   623  ...     38:37                                                   \n","9     5   569  ...     39:24   \\n\\r\\n                            INTEL TEAM ...\n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":141}]},{"cell_type":"markdown","metadata":{"id":"lKaslkO22y38"},"source":["Removing any white space"]},{"cell_type":"code","metadata":{"id":"8uvKu0qF22cX"},"source":["df7.replace(r'\\s', '', regex = True, inplace = True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tBWJ_JUS1PEN"},"source":["And getting rid of the new line characters"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"CKm_GTPr1Ss1","executionInfo":{"status":"ok","timestamp":1627303961308,"user_tz":300,"elapsed":268,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"05123eef-628b-4582-c9f7-97cdeb8e9251"},"source":["df8 = df7.replace(r'\\\\n',' ', regex=True) \n","df8.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Place</th>\n","      <th>Bib</th>\n","      <th>Name</th>\n","      <th>Gender</th>\n","      <th>City</th>\n","      <th>State</th>\n","      <th>Chip Time</th>\n","      <th>Chip Pace</th>\n","      <th>Gun Time</th>\n","      <th>Team</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>814</td>\n","      <td>JAREDWILSON</td>\n","      <td>M</td>\n","      <td>TIGARD</td>\n","      <td>OR</td>\n","      <td>36:21</td>\n","      <td>5:51</td>\n","      <td>36:24</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2</td>\n","      <td>573</td>\n","      <td>NATHANASUSTERSIC</td>\n","      <td>M</td>\n","      <td>PORTLAND</td>\n","      <td>OR</td>\n","      <td>36:42</td>\n","      <td>5:55</td>\n","      <td>36:45</td>\n","      <td>INTELTEAMF</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>3</td>\n","      <td>687</td>\n","      <td>FRANCISCOMAYA</td>\n","      <td>M</td>\n","      <td>PORTLAND</td>\n","      <td>OR</td>\n","      <td>37:44</td>\n","      <td>6:05</td>\n","      <td>37:48</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>4</td>\n","      <td>623</td>\n","      <td>PAULMORROW</td>\n","      <td>M</td>\n","      <td>BEAVERTON</td>\n","      <td>OR</td>\n","      <td>38:34</td>\n","      <td>6:13</td>\n","      <td>38:37</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>5</td>\n","      <td>569</td>\n","      <td>DEREKGOSBORNE</td>\n","      <td>M</td>\n","      <td>HILLSBORO</td>\n","      <td>OR</td>\n","      <td>39:21</td>\n","      <td>6:20</td>\n","      <td>39:24</td>\n","      <td>INTELTEAMF</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Place  Bib              Name  ...  Chip Pace  Gun Time        Team\n","5     1  814       JAREDWILSON  ...       5:51     36:24            \n","6     2  573  NATHANASUSTERSIC  ...       5:55     36:45  INTELTEAMF\n","7     3  687     FRANCISCOMAYA  ...       6:05     37:48            \n","8     4  623        PAULMORROW  ...       6:13     38:37            \n","9     5  569     DEREKGOSBORNE  ...       6:20     39:24  INTELTEAMF\n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":144}]},{"cell_type":"markdown","metadata":{"id":"1WHzyz4TzEPG"},"source":["It took a while to get here, but at this point, the dataframe is in the desired format. \n","\n","If you would like to read about another webscraping project, take a look at [this blog post about scraping a job portal](https://realpython.com/beautiful-soup-web-scraper-python/). This is about getting data from the [Fake Python Jobs](https://realpython.github.io/fake-jobs/) site (**NOTE**: These are **FAKE** posts; the jobs **don't exist**; this is a site built exclusively for static HTML-based web scraping). How is that for hunting for your dream job?"]},{"cell_type":"markdown","metadata":{"id":"F-Wxc4tW40Sx"},"source":["# **2. Working with an API**\n","Getting information directly from webpages is one thing--in fact, a lot of online  marketing companies specialize in scraping and cleaning data and then sell these data to other businesses for further analysis. As you may already guess, this works only with static HTML sites, that is, with sites that send your browser complete webpages, not just shells of css and javascript or Ajax with a database call in the middle. And, as you have seen, this can be very painful.\n","\n","That's why some website providers offer application programming interfaces (APIs) that allow you to access their data in a predefined manner. With APIs, you can avoid parsing HTML. Instead, you can access the data directly using formats like JSON and XML. \n","\n","When you use an API, the process is generally more stable than gathering the data through web scraping. That’s because developers create APIs to be consumed by programs rather than by human eyes."]},{"cell_type":"markdown","metadata":{"id":"VIkNTYwC7_9M"},"source":["## **2.1 Setting up the Data Source**\n","In order to work with an API, the first step is always to obtain the required login credentials into the source of your data and store these in an **APP** (yes, I said app because that's what this is called--no relation to whatever you have running in your cellphone). Imagine this like getting the key to your house or apartment from your landlord or realtor:\n","\n","\n","### **2.1.1 API #1: New York Times**\n","\n","\n","Let's assume we work with the **New York Times** API:\n","1. Go to [the New York Times API website](https://developer.nytimes.com/get-started) and sign up for an account. \n","2. Once you have completed the email verification process, log into the website and start setting your API key up under Get Started (see below). This is the access tool that your Python code needs in order to download data from the API.\n","<div>\n","<center>\n","<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/nytapi.JPG\" width=\"500\">\n","</div>\n","4. The instructions ask you to create an app. No worries: That is API-speak for a set of dedicated access keys that allow you to use the API. After you log in, go to Get Started and follow the app generation procedure. Enable the Article Search, the Community API, and the Top Stories API and don't forget to hit \"save.\"\n","5. This will give you an app ID and a set of keys. \n","<div>\n","<center>\n","<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/nytapi_keys.JPG\" width=\"500\">\n","</div>\n","6. Once you know that you have these available, go to the APIs and learn about how to connect to each of them. Be sure that the API you wish to use is **AUTHORIZED** in your app.\n","\n","Now you have set up ONE side of the puzzle--**the API side.**\n","\n","To learn more about how to connect into the New York Times API, check out this [blog post](https://dlab.berkeley.edu/blog/scraping-new-york-times-articles-python-tutorial) or [this (somewhat older) notebook](https://github.com/nilmolne/Text-Mining-The-New-York-Times-Articles/blob/master/Code/HowToUse.ipynb) or this [notebook about COVID-related articles](https://github.com/brienna/coronavirus-news-analysis/blob/master/2020_05_01_get_data_from_NYT.ipynb).\n","\n","\n","### **2.1.2 API #2: Reddit**\n","\n","The principle here is the same as before: Build a user account and configure your app, then write down your key(s) because you'll need it/ them when your code wants to connect to the data source. Here is how this works on Reddit, complete with demonstration:\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335},"id":"Amv1v0Kydhli","executionInfo":{"status":"ok","timestamp":1627448149343,"user_tz":300,"elapsed":181,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"2fa98429-a203-483e-c963-049901cc28e9"},"source":["from IPython.display import HTML\n","HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/FdjVoOf9HN4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/FdjVoOf9HN4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"jQbvTfq7eLS1"},"source":["### **2.1.3 API #3: Twitter**\n","\n","Social Media APIs are the hardest to get access to. Right now, in order to qualify for an app on Twitter, you have to submit detailed explanations about how you will access the data, how you will store them, and how you will analyze them. In contrast to the New York Times API, where authorization is automated and instant, Twitter, just like Facebook, Instagram, and others, has actual human employees review and approve your application. That's because ever since the [Cambridge Analytica scandal](https://en.wikipedia.org/wiki/Cambridge_Analytica) and with the strict enforcement of [GDPR](https://gdpr.eu/), social networks can get into **a lot** of very expensive trouble if they do not protect user data well. \n","\n","Read [here](https://developer.twitter.com/en/products/twitter-api/standard) about Twitter's policies for using their API and check out the requirements for [applying for a \"developer account,\"](https://developer.twitter.com/en/portal/petition/use-case) which is the platform on which you would build your app. [These instructions](https://developer.twitter.com/en/docs/tutorials/step-by-step-guide-to-making-your-first-request-to-the-twitter-api-v2) will walk you through the process step-by-step\n","\n","**HERE ARE SOME OF THE QUESTIONS YOU WILL HAVE TO ANSWER** if you apply for API access:\n","\n","* How will you use the Twitter API or Twitter Data?\n","  * In English, please describe how you plan to use Twitter data and/or APIs. The more detailed the response, the easier it is to review and approve. Please be thoughtful and thorough\n","\n","* Please answer each of the following with as much detail and accuracy as possible. Failure to do so could result in delays to your access to Twitter developer platform or rejected applications.\n","  * Are you planning to analyze Twitter data? Please describe how you will analyze Twitter data including any analysis of Tweets or Twitter users. Please be thoughtful and thorough\n","* Will your app use Tweet, Retweet, Like, Follow, or Direct Message functionality?\n","  * Please describe your planned use of these features. Please be thoughtful and thorough\n","* Do you plan to display Tweets or aggregate data about Twitter content outside Twitter?\n","  * Please describe how and where Tweets and/or data about Twitter content will be displayed outside of Twitter. Please be thoughtful and thorough\n","* Will your product, service, or analysis make Twitter content or derived information available to a government entity?\n","  * Please list all government entities you intend to provide Twitter content or derived information to under this use case.\n","\n","Once you have written the Great American Novel for each of these fields, submitted, and verified your email address, you'll get here:\n","\n","<div>\n","<center>\n","<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/twitter.JPG\" width=\"500\">\n","</div>\n","\n","**FINALLY**, you'll be routed to a set of tutorials which provide you with the code stubs for [sentiment analysis](https://developer.twitter.com/en/docs/tutorials/how-to-analyze-the-sentiment-of-your-own-tweets) or time series analysis or any other analysis you could wish for. \n","\n","That's a lot of work, isn't it?"]},{"cell_type":"markdown","metadata":{"id":"lODGk1xAEZTR"},"source":["## **2.2 Setting up Python**\n","Now that you have set up the API of your choice, the second part of the project is to connect to it and retrieve the data. This means that, now that you have the key(s) available, you write your Colab or Jupyter Notebook code to use these keys to log into the API. This comes in different flavors, as well.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"b-4V9so0DdeP"},"source":["### **2.2.1 New York Times**\n","So, to connect to the **New York Times**, the simplest code can look like what you are seeing below, with the requests, os, an pprint libraries. Note that  NYTIMES_APIKEY is the key that you received when signing up for the New York Times APIs (see screenshot above). Here you see the Top Story API in action:*italicized text*"]},{"cell_type":"code","metadata":{"id":"Nc-F6bv5-sk3"},"source":["import requests\n","import os\n","from pprint import pprint\n","\n","apikey = os.getenv('NYTIMES_APIKEY', '...')\n","\n","# Top Stories:\n","# https://developer.nytimes.com/docs/top-stories-product/1/overview\n","section = \"science\"\n","query_url = f\"https://api.nytimes.com/svc/topstories/v2/{section}.json?api-key={apikey}\"\n","\n","r = requests.get(query_url)\n","pprint(r.json())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lM66dfuk_YQi"},"source":["The snippet above is very straightforward. We run a GET request against topstories/v2 endpoint supplying section name and our API key. \n","\n","The output comes in JSON format and looks like this:\n","```\n","{ 'last_updated': '2020-08-09T08:07:44-04:00',\n"," 'num_results': 25,\n"," 'results': [{'abstract': 'New Zealand marked 100 days with no new reported '\n","                          'cases of local coronavirus transmission. France '\n","                          'will require people to wear masks in crowded '\n","                          'outdoor areas.',\n","              'byline': '',\n","              'created_date': '2020-08-09T08:00:12-04:00',\n","              'item_type': 'Article',\n","              'multimedia': [{'caption': '',\n","                              'copyright': 'The New York Times',\n","                              'format': 'superJumbo',\n","                              'height': 1080,\n","                              'subtype': 'photo',\n","                              'type': 'image',\n","                              'url': 'https://static01.nyt.com/images/2020/08/03/us/us-briefing-promo-image-print/us-briefing-promo-image-superJumbo.jpg',\n","                              'width': 1920},\n","                             ],\n","              'published_date': '2020-08-09T08:00:12-04:00',\n","              'section': 'world',\n","              'short_url': 'https://nyti.ms/3gH9NXP',\n","              'title': 'Coronavirus Live Updates: DeWine Stresses Tests’ '\n","                       'Value, Even After His False Positive',\n","              'uri': 'nyt://article/27dd9f30-ad63-52fe-95ab-1eba3d6a553b',\n","              'url': 'https://www.nytimes.com/2020/08/09/world/coronavirus-covid-19.html'},\n","             ]\n"," }\n","\n","```\n","That is the shortest API call. The article API gives you more filtering options. The only mandatory field is q (query), which is the search term. Beyond that you can mix and match filter query, date range ( begin_date, end_date), page number, sort order and facet fields. \n","```\n","# Article Search:\n","# https://api.nytimes.com/svc/search/v2/articlesearch.json?q=<QUERY>&api-key=<APIKEY>\n","# Use - https://developer.nytimes.com/docs/articlesearch-product/1/routes/articlesearch.json/get to explore API\n","\n","query = \"politics\"\n","begin_date = \"20200701\"  # YYYYMMDD\n","filter_query = \"\\\"body:(\\\"Trump\\\") AND glocations:(\\\"WASHINGTON\\\")\\\"\"  # http://www.lucenetutorial.com/lucene-query-syntax.html\n","page = \"0\"  # <0-100>\n","sort = \"relevance\"  # newest, oldest\n","query_url = f\"https://api.nytimes.com/svc/search/v2/articlesearch.json?\" \\\n","            f\"q={query}\" \\\n","            f\"&api-key={apikey}\" \\\n","            f\"&begin_date={begin_date}\" \\\n","            f\"&fq={filter_query}\" \\\n","            f\"&page={page}\" \\\n","            f\"&sort={sort}\"\n","\n","r = requests.get(query_url)\n","pprint(r.json())\n","```\n","The final challenge is to transform the JSON content into a file that can serve as a data frame. \n","\n","To learn more about newspaper APIs, check out [Martin Heinz' blog post](https://martinheinz.dev/blog/31) about how to connect to them.\n"]},{"cell_type":"markdown","metadata":{"id":"kYxW5V4ADldr"},"source":["### **2.2.2 Reddit**\n","Connecting with **Reddit** is explained really well in [this article](https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c), which the code in this section summarizes. Also, you need more than just one token this time around:\n","1. Your Reddit Login and password\n","2. The personal use script key you receive when you sign up for your app\n","3. The secret key that you receive when you sign up for your app\n","\n","Once you have these, you will need to set up your OAuth configuration. This will assign to you a token that expires every 2 hours:\n","\n","```\n","import requests\n","\n","# note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n","auth = requests.auth.HTTPBasicAuth('<CLIENT_ID>', '<SECRET_TOKEN>')\n","\n","# here we pass our login method (password), username, and password\n","data = {'grant_type': 'password',\n","        'username': '<USERNAME>',\n","        'password': '<PASSWORD>'}\n","\n","# setup our header info, which gives reddit a brief description of our app\n","headers = {'User-Agent': 'MyBot/0.0.1'}\n","\n","# send our request for an OAuth token\n","res = requests.post('https://www.reddit.com/api/v1/access_token',\n","                    auth=auth, data=data, headers=headers)\n","\n","# convert response to JSON and pull access_token value\n","TOKEN = res.json()['access_token']\n","\n","# add authorization to our headers dictionary\n","headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n","\n","# while the token is valid (~2 hours) we just add headers=headers to our requests\n","requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)\n","```\n","That is the first part. Now the API trusts us, and we can retrieve text. First, we'll look at the most popular posts:\n","```\n","res = requests.get(\"https://oauth.reddit.com/r/python/hot\",\n","                   headers=headers)\n","\n","print(res.json())  # This will pull out all the \"hot\" posts\n","```\n","The result will be a big ugly JSON-formatted set. Think about it as a bag of words.\n","\n","\n","---\n","\n","\n","Let's inspect this bag of words in order to find the most interesting posts about--what else?--cats. First, we'll explore the titles of the retrieved posts:\n","```\n","for post in res.json()['data'] ['cats']:\n","   print(post['data'] ['title']\n","```\n","There is always enough material on Reddit about cats! So, let's build this search as a filter into our data retrieval query **AND** collect the output in a dataframe (which we will name \"fluffy\"):\n","\n","```\n","# make a request for the trending posts in /r/Python\n","res = requests.get(\"https://oauth.reddit.com/r/python/hot\",\n","                   headers=headers)\n","\n","fluffy = pd.DataFrame()  # initializing our dataframe\n","\n","# loop through each post retrieved from GET request\n","for post in res.json()['data']['cats']:\n","    # here, we append relevant data to our dataframe\n","    df = df.append({\n","        'subreddit': post['data']['subreddit'],\n","        'title': post['data']['title'],\n","        'selftext': post['data']['selftext'],\n","        'upvote_ratio': post['data']['upvote_ratio'],\n","        'ups': post['data']['ups'],\n","        'downs': post['data']['downs'],\n","        'score': post['data']['score']\n","    }, ignore_index=True)\n","\n","```\n","---\n","Afterwards, inspect the dataframe with fluffy.head(), and you'll see all the new posts about cats already in a dataframe. Follow the steps in part 1 of this workbook to clean your data up a little, including removing URLs and any special characters, and  your data is ready for analysis!\n"]},{"cell_type":"markdown","metadata":{"id":"arRFvDaBKmvq"},"source":["### **2.2.3 Twitter**\n","Connecting to any high-visibility social media is more involved these days due to data privacy concerns--even if the data is as public as on Twitter (trying to get the Facebook or Instagram APIs set up and calls working can take days, mostly for approval turnarounds).\n","\n","BUT, assuming that you have your logins and your tokens available, the Twitter API is easily managed. Twitter has published [a treasure trove of code stubs](https://github.com/twitterdev/Twitter-API-v2-sample-code) on GitHub for that purpose. \n","\n","The most interesting code snippets for our purpose are:\n","* [Full archive search](https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/main/Full-Archive-Search/full-archive-search.py)\n","* [User Tweet timeline](https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/main/User-Tweet-Timeline/user_tweets.py)\n","\n","Twitter also allows you to connect with the [Postman package](https://developer.twitter.com/en/docs/tools-and-libraries/using-postman), which uses HTTP to retrieve data through a GUI (of course, any true hacker wouldn't be caught dead using a GUI).\n","\n","If you already have Twitter API access, I encourage you to explore the options here. If not, the NYT or the Reddit APIs might be easier for you to work with.\n"]},{"cell_type":"markdown","metadata":{"id":"vHrhKCKfOG7p"},"source":["## Your Turn\n","In this workbook, you have encountered 2 major methods of obtaining data: Either through direct webscraping (section 1) or through the use of an API (section 2). One of the big takeaways here is that all APIs behave slightly differently, but the process has several steps in common:\n","1. Register for a user account on the website\n","2. Build your app\n","3. Note your keys, which you will need to connect\n","4. Pivot to your notebook\n","5. Install at a minimum the requests and os packages\n","6. Set up variables to hold your keys\n","7. Test the authentication method\n","8. Write your query code--test\n","9. Edit your query code to pull data into a dataframe--test\n","10. Clean and format the data\n","Pick one of these methods or one of the APIs with which you want to work. Then see what data you can pull down. Try formatting the data. If you would like more help converting JSON output to a pandas dataframe than you are seeing above, [this article](https://towardsdatascience.com/how-to-convert-json-into-a-pandas-dataframe-100b2ae1e0d8) will walk you through the individual steps."]},{"cell_type":"markdown","metadata":{"id":"1hBB9E9mQofE"},"source":[""]}]}