{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"Module4_DataWrangling2_Transformation.ipynb","provenance":[{"file_id":"13gMg_WLEhNpJ4s16i5YH0mBYDHfX7ocn","timestamp":1622296550607}],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"-mh679lW5v_t"},"source":["#**Module 4: Data Wrangling Part 2**\n","In this module, you will learn how to\n","* Transform data\n","* De-duplicate data\n","* Substitute data\n","* Discretize data\n","\n","**Be sure to expand all the hidden cells, run all the code, and do all the exercises--you will need the techniques for the lesson lab!**"]},{"cell_type":"markdown","metadata":{"id":"7-M0cWUsbPO2"},"source":["Remember the [Rocky and Bullwinkle video](https://youtu.be/kx3sOqW5zj4)? In this module, you will learn about how to change the lion into the tiger into the bear into the rhino. You will also learn how to summarize the lion and the tiger into a new \"big cats\" category"]},{"cell_type":"markdown","metadata":{"id":"bnVNWu8SBjIg"},"source":["#**0. Preparation and Setup**\n","We are working with our adult dataset again, so we're loading our libraries and our dataset just like last time."]},{"cell_type":"code","metadata":{"id":"vui8bv375v_v","colab":{"base_uri":"https://localhost:8080/","height":197},"executionInfo":{"status":"ok","timestamp":1622386100476,"user_tz":300,"elapsed":1132,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"5d0e3b1f-db5a-4484-cbf5-2bd10ef5063f"},"source":["import numpy as np\n","import pandas as pd \n","import matplotlib.pyplot as plt\n","\n","#Reading in the data as adult dataframe\n","adult = pd.read_csv(\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/data/adult.data.simplified.csv\")\n","\n","#Verifying that we can see the data\n","adult.head()"],"execution_count":242,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>workclass</th>\n","      <th>education</th>\n","      <th>educationyears</th>\n","      <th>maritalstatus</th>\n","      <th>occupation</th>\n","      <th>relationship</th>\n","      <th>race</th>\n","      <th>sex</th>\n","      <th>hoursperweek</th>\n","      <th>nativecountry</th>\n","      <th>incomeUSD</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>39</td>\n","      <td>State-gov</td>\n","      <td>Bachelors</td>\n","      <td>13</td>\n","      <td>Never-married</td>\n","      <td>Adm-clerical</td>\n","      <td>Not-in-family</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>40</td>\n","      <td>United-States</td>\n","      <td>43747</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>50</td>\n","      <td>Self-emp-not-inc</td>\n","      <td>Bachelors</td>\n","      <td>13</td>\n","      <td>Married-civ-spouse</td>\n","      <td>Exec-managerial</td>\n","      <td>Husband</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>13</td>\n","      <td>United-States</td>\n","      <td>38907</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>38</td>\n","      <td>Private</td>\n","      <td>HS-grad</td>\n","      <td>9</td>\n","      <td>Divorced</td>\n","      <td>Handlers-cleaners</td>\n","      <td>Not-in-family</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>40</td>\n","      <td>United-States</td>\n","      <td>25055</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>53</td>\n","      <td>Private</td>\n","      <td>11th</td>\n","      <td>7</td>\n","      <td>Married-civ-spouse</td>\n","      <td>Handlers-cleaners</td>\n","      <td>Husband</td>\n","      <td>Black</td>\n","      <td>Male</td>\n","      <td>40</td>\n","      <td>United-States</td>\n","      <td>26733</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>28</td>\n","      <td>Private</td>\n","      <td>Bachelors</td>\n","      <td>13</td>\n","      <td>Married-civ-spouse</td>\n","      <td>Prof-specialty</td>\n","      <td>Wife</td>\n","      <td>Black</td>\n","      <td>Female</td>\n","      <td>40</td>\n","      <td>Cuba</td>\n","      <td>23429</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   age         workclass  education  ...  hoursperweek  nativecountry incomeUSD\n","0   39         State-gov  Bachelors  ...            40  United-States     43747\n","1   50  Self-emp-not-inc  Bachelors  ...            13  United-States     38907\n","2   38           Private    HS-grad  ...            40  United-States     25055\n","3   53           Private       11th  ...            40  United-States     26733\n","4   28           Private  Bachelors  ...            40           Cuba     23429\n","\n","[5 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":242}]},{"cell_type":"markdown","metadata":{"id":"gQDK8kjYdqCl"},"source":["And here is the adult_small dataframe that you built in the previous notebook. We will use it for a small, fast, proof-of-concept before we use more resources for the larger computation."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":464},"id":"eF80q8WMc7Zh","executionInfo":{"status":"ok","timestamp":1622386102127,"user_tz":300,"elapsed":1358,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"d926a3f7-8b49-45d7-edfb-0b67dcfa631e"},"source":["adult_small=adult.iloc[70:80,]\n","adult_small"],"execution_count":243,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>workclass</th>\n","      <th>education</th>\n","      <th>educationyears</th>\n","      <th>maritalstatus</th>\n","      <th>occupation</th>\n","      <th>relationship</th>\n","      <th>race</th>\n","      <th>sex</th>\n","      <th>hoursperweek</th>\n","      <th>nativecountry</th>\n","      <th>incomeUSD</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>70</th>\n","      <td>19</td>\n","      <td>Private</td>\n","      <td>Some-college</td>\n","      <td>10</td>\n","      <td>Never-married</td>\n","      <td>Prof-specialty</td>\n","      <td>Own-child</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>32</td>\n","      <td>United-States</td>\n","      <td>41013</td>\n","    </tr>\n","    <tr>\n","      <th>71</th>\n","      <td>31</td>\n","      <td>Private</td>\n","      <td>Bachelors</td>\n","      <td>13</td>\n","      <td>Separated</td>\n","      <td>Sales</td>\n","      <td>Own-child</td>\n","      <td>Black</td>\n","      <td>Female</td>\n","      <td>40</td>\n","      <td>United-States</td>\n","      <td>33571</td>\n","    </tr>\n","    <tr>\n","      <th>72</th>\n","      <td>29</td>\n","      <td>Self-emp-not-inc</td>\n","      <td>Bachelors</td>\n","      <td>13</td>\n","      <td>Married-civ-spouse</td>\n","      <td>Sales</td>\n","      <td>Husband</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>70</td>\n","      <td>United-States</td>\n","      <td>190337</td>\n","    </tr>\n","    <tr>\n","      <th>73</th>\n","      <td>23</td>\n","      <td>Private</td>\n","      <td>Some-college</td>\n","      <td>10</td>\n","      <td>Never-married</td>\n","      <td>Machine-op-inspct</td>\n","      <td>Not-in-family</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>40</td>\n","      <td>United-States</td>\n","      <td>23773</td>\n","    </tr>\n","    <tr>\n","      <th>74</th>\n","      <td>79</td>\n","      <td>Private</td>\n","      <td>Some-college</td>\n","      <td>10</td>\n","      <td>Married-civ-spouse</td>\n","      <td>Prof-specialty</td>\n","      <td>Other-relative</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>20</td>\n","      <td>United-States</td>\n","      <td>49908</td>\n","    </tr>\n","    <tr>\n","      <th>75</th>\n","      <td>27</td>\n","      <td>Private</td>\n","      <td>HS-grad</td>\n","      <td>9</td>\n","      <td>Never-married</td>\n","      <td>Other-service</td>\n","      <td>Own-child</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>40</td>\n","      <td>Mexico</td>\n","      <td>34334</td>\n","    </tr>\n","    <tr>\n","      <th>76</th>\n","      <td>40</td>\n","      <td>Private</td>\n","      <td>Assoc-acdm</td>\n","      <td>12</td>\n","      <td>Married-civ-spouse</td>\n","      <td>Adm-clerical</td>\n","      <td>Husband</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>40</td>\n","      <td>United-States</td>\n","      <td>47470</td>\n","    </tr>\n","    <tr>\n","      <th>77</th>\n","      <td>67</td>\n","      <td>?</td>\n","      <td>10th</td>\n","      <td>6</td>\n","      <td>Married-civ-spouse</td>\n","      <td>?</td>\n","      <td>Husband</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>2</td>\n","      <td>United-States</td>\n","      <td>40188</td>\n","    </tr>\n","    <tr>\n","      <th>78</th>\n","      <td>18</td>\n","      <td>Private</td>\n","      <td>11th</td>\n","      <td>7</td>\n","      <td>Never-married</td>\n","      <td>Other-service</td>\n","      <td>Own-child</td>\n","      <td>White</td>\n","      <td>Female</td>\n","      <td>22</td>\n","      <td>United-States</td>\n","      <td>44387</td>\n","    </tr>\n","    <tr>\n","      <th>79</th>\n","      <td>31</td>\n","      <td>Local-gov</td>\n","      <td>7th-8th</td>\n","      <td>4</td>\n","      <td>Married-civ-spouse</td>\n","      <td>Farming-fishing</td>\n","      <td>Husband</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>40</td>\n","      <td>United-States</td>\n","      <td>40938</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    age         workclass     education  ...  hoursperweek  nativecountry incomeUSD\n","70   19           Private  Some-college  ...            32  United-States     41013\n","71   31           Private     Bachelors  ...            40  United-States     33571\n","72   29  Self-emp-not-inc     Bachelors  ...            70  United-States    190337\n","73   23           Private  Some-college  ...            40  United-States     23773\n","74   79           Private  Some-college  ...            20  United-States     49908\n","75   27           Private       HS-grad  ...            40         Mexico     34334\n","76   40           Private    Assoc-acdm  ...            40  United-States     47470\n","77   67                 ?          10th  ...             2  United-States     40188\n","78   18           Private          11th  ...            22  United-States     44387\n","79   31         Local-gov       7th-8th  ...            40  United-States     40938\n","\n","[10 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":243}]},{"cell_type":"markdown","metadata":{"id":"zymnCB2E-szM"},"source":["# **1. Data Transformation**\n","As you have seen, numeric datatypes allow you to do much more fun math than string datatypes. You can count them, sum them, average them, boxplot them ... all that, while string datatypes, well they can be counted, so much is true--but that really is it. "]},{"cell_type":"markdown","metadata":{"id":"PJQHnuM45wBb"},"source":["\n","We want to transform a categorical attribute, in this case 'race', into a numeric datatype. The Python format for this operation is `DataFrame.astype(dtype, copy=True, errors='raise')`\n","In our case, this would be something like \n","* `adult_small.race.astype('int32')`\n","* `adult_small.race.astype('category')`\n","* `adult_small.race.astype('category', ordered=True)`\n","\n","Let's get started. First, let's verify our datatype for 'race'\n","\n"]},{"cell_type":"code","metadata":{"id":"nzXWvcb35wBd"},"source":["adult_small.race.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ERq9GIGtcACB"},"source":["'O' means 'object', which is basically a string value. Let's now transform our 'race' attribute to numeric values. Below are all the steps."]},{"cell_type":"markdown","metadata":{"id":"HocgWNiGqcIZ"},"source":["## **1.1 Adding a column with the data we want to transform**\n","We could theoretically transform the 'race' attribute in place, but if our code causes problems, we could mess up our entire dataframe. So, instead, we will first add a new column into which we will copy the contents of the 'race' attribute. In a second step, we will transform the values in that new column.\n","\n","Detailed explanations are in the code comments below."]},{"cell_type":"code","metadata":{"id":"Zpk1SU9w5wBu"},"source":["# Creating a new attribute and populating it with the contents of the attribute that we want to transform.\n","race_num=adult_small.race\n","\n","# Adding the new attribute to the dataframe\n","adult_small['race_num'] = race_num\n","\n","# Checking the contents of the new attribute\n","adult_small.race_num"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5c6aiJYUmwOn"},"source":["# Let's check the shape now. Instead of 12 attributes, we should have 10 rows and 13 columns now because we have built a new race_num attribute\n","adult_small.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eJQe0tGsl7i-"},"source":["# Now that we have the race_num column built, let's check the datatypes of both attributes. \n","adult_small.dtypes[['race','race_num']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4KgdeN0Mqp9j"},"source":["## **1.2 Transforming the values**\n","To transform an object value to numeric, we need to get the numbers from somewhere. In our case, we will transform the object to category. That gives us the index numbers for the category. Then we will replace the string values with these index numbers for the category--and voila! we have a numeric transformation.\n","(If you see any attribute replacement warnings as you run the code below, you can ignore them.)"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"xLD7OLlm5wB1"},"source":["# Here, we convert the race_num values to categorical (to obtain the index numbers) \n","adult_small['race_num']= adult_small['race_num'].astype('category')\n","\n","# Now we replace the categories with their index numbers, i.e. the category codes\n","adult_small['race_num']= adult_small['race_num'].cat.codes\n","\n","# Let's see what the datatypes look like now\n","adult_small.dtypes[['race','race_num']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7xL7eGHW5wB8"},"source":["# Now let's display the contents of both attributes to double-check \n","adult_small[['race','race_num']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QDA_qvRzsOQX"},"source":["**SUCCESS!** Now that we have worked the code out for our small proof-of-concept dataframe, we can apply it to the entire adult dataframe."]},{"cell_type":"code","metadata":{"id":"7o-UELnu5wCG"},"source":["# First, we back the dataframe up.\n","adult2=adult\n","\n","# Now we build the column and copy the original values into it\n","adult2['race_num'] = adult2.race\n","\n","# Third, we convert race_num into categorical\n","adult2['race_num']= adult2['race_num'].astype('category')\n","\n","# Then, we replace the strings with the category indices\n","adult2['race_num']= adult2['race_num'].cat.codes\n","\n","# Now, we check the data types\n","adult2.dtypes[['race','race_num']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PgGqvx7QtehQ"},"source":["**YAY!** That worked, too. But how did pandas assign the numbers? Let's find out:"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"FwGxPCDp5wCN"},"source":["# Making a temporary new dataframe because it's so much easier to work with just 2 attributes\n","race_exploration=adult2[['race','race_num']]\n","race2=race_exploration.drop_duplicates()\n","race2.sort_values(by=['race_num'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FRDyehO-n-7g"},"source":["##**Your Turn**\n","Explain in a couple of complete sentences the logic with which pandas haa ssigned the numbers for the race_num array."]},{"cell_type":"markdown","metadata":{"id":"dfLScvE35wCU"},"source":["# **2. De-Duplication**\n","Check out this line in the example above:\n","`race2=race_exploration.drop_duplicates()`\n","\n","This is literally all there is to de-duplication. \n","For more on drop_duplicates() see, https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html"]},{"cell_type":"markdown","metadata":{"id":"DaQm7prI5wCV"},"source":["# **3. Substituting Values in Dataframes**\n","\n","OK--tiger, meet lion, and lion--meet rhino! Here is how to substitute values in a dataframe. For this purpose, we will try out the df.replace method shown in https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html. This really convenient method works for strings and numeric values alike.\n","\n","Here, we will work with the workclass attribute."]},{"cell_type":"code","metadata":{"id":"SDfB_pNT5wCW"},"source":["adult3 = adult\n","adult3.workclass.unique()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NnX1383hzPRl"},"source":["### The Problem\n","Oh, look: We have a '?' value in there. This could be any of the following\n","* Misformatted data that might have accidentally broken during the E of [ETL](https://www.sas.com/en_us/insights/data-management/what-is-etl.html). \n","* Some sort of placeholder character that our data warehouse has inserted for missing values.\n","* Intentional and supposed to stand for 'Unknown'. \n","\n","Whatever it is, we may never know. That is one of the big problems when dealing with big datasets. The next problem is:\n","**bold text**\n","> **What are we going to do with it?**\n","\n","Are we going to just **ignore** the rows in which the '?' appears--or are we going to **do something about it**? To answer that question, it is helpful to know how many rows contain the '?'\n","\n"]},{"cell_type":"code","metadata":{"id":"epXscXUH5wCd"},"source":["adult3[adult3['workclass'] == '?']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p8nxCFHy2hBF"},"source":["Hm. That's 1,836 rows out of 32,561--almost 6% of all the data. At this point, it's a good idea to ask your supervisor, that is, the client of your analysis, what they would prefer. Let's say the client comes back to say they **don't** want you to drop 6% of the dataset. \n","\n","At that point, you will have to make an assumption--and that's where critical thinking skills come in: Does it matter whether there's an accidental or an intentional '?'. If it does, then further research is needed. If it does NOT, then it might be safe to assume that the '?' stands for 'Unknown'. \n","\n","That's why we will translate '?' to 'Unknown'."]},{"cell_type":"code","metadata":{"id":"Cg9UwBDC5wCj"},"source":["# Instead of building another column to see any transformations, we will substitute data in place this time around.\n","# For this purpose, we're using the replace() method from pandas\n","adult3.workclass.replace('?','Unknown', inplace=True)\n","\n","# Did we indeed replace all '?'? Let's check:\n","adult3[adult3['workclass'] == '?']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ORwHoBmN5wCo"},"source":["# Looks like it worked. Let's check if the new value appears in the list of unique values.\n","adult3.workclass.unique()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zn3ggAxc4mnJ"},"source":["## **Your Turn**\n","Take a look at the unique values in the occupation attribute. Do you see something similar as what we solved above?\n","1. Replace the unusual value with 'NA'\n","2. Then count how many rows contain 'NA' values"]},{"cell_type":"code","metadata":{"id":"bNMl3Ud_5wCz","executionInfo":{"status":"ok","timestamp":1622386102250,"user_tz":300,"elapsed":424,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":[""],"execution_count":255,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"49JiTxR35wDy"},"source":["# **4. Discretization**\n","Remember how I said we're also going to exchange the lion and the tiger from the Rocky and Bullwinkle cartoon into a \"Big Cats\" category? That's essentially what Discretization is about.\n","\n","Data discretization is typically defined as a process of converting continuous data attribute values into a finite set of intervals and associating with each interval some specific data value. So, you could say that the goal of discretization is to simplify data by putting them into summary categories. While discretization is typically applied to numeric data only, we can also use this concept to simplify other data (like object-type data or data that has too many categories to be useful). \n"]},{"cell_type":"markdown","metadata":{"id":"ryIUaek080A5"},"source":["##**4.1 Data Summarization**\n","We're not done with the workclass attribute just yet. As you have seen, we have 9 categories. We will reduce these to the following:\n","- Unknown--contains all Unknown, Never-worked, and Without-pay\n","- Government--contains all Federal, State, and Local government employees\n","- Self--contains all self-employed groups\n","\n","This summarization will allow us to more easily group our data by the three larger units in the workclass attribute."]},{"cell_type":"code","metadata":{"id":"0RmNwoXC5wDz"},"source":["# Let's check the categories in the workclass attribute!\n","adult3.workclass.unique()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d62mRQex5wD_"},"source":["Given the structure of this attribute, we are seeing three \"natural\" groups: Unknown, Government, and self-employed people. This means we can simplify the data we have and create three big aggregations with simple substitutions:"]},{"cell_type":"code","metadata":{"id":"zyEPwKqY5wEA","executionInfo":{"status":"ok","timestamp":1622386102269,"user_tz":300,"elapsed":426,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":["#Summarizing all the Unknowns\n","adult3.workclass.replace('Unknown','Unknown', inplace=True)\n","adult3.workclass.replace('Never-worked','Unknown', inplace=True)\n","adult3.workclass.replace('Without-pay','Unknown', inplace=True)"],"execution_count":257,"outputs":[]},{"cell_type":"code","metadata":{"id":"xqvKM57l8yxt","executionInfo":{"status":"ok","timestamp":1622386102272,"user_tz":300,"elapsed":426,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":["#Summarizing all the Government attributes\n","adult3.workclass.replace('State-gov', 'Government', inplace=True) \n","adult3.workclass.replace('Federal-gov', 'Government', inplace=True)\n","adult3.workclass.replace('Local-gov', 'Government', inplace=True)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           "],"execution_count":258,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xTGaIHBom8O2"},"source":["##**Your Turn**\n","Complete the job! Summarize the other three groups into the \"Self\" category."]},{"cell_type":"code","metadata":{"id":"ODdtCWvLnq6f","executionInfo":{"status":"ok","timestamp":1622386102275,"user_tz":300,"elapsed":416,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":[""],"execution_count":258,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tDpoPQdC7Q6C"},"source":["##**4.2 Binning and Bucketing (Numeric Discretization)**\n","We have arrived at the \"pure\" meaning of discretization--the one you find in most textbooks. In this context, you'll often hear the terms binning and bucketing. \n","\n","In short, Data binning and bucketing groups data in bins or buckets, in the sense that it replaces values contained into a small interval with a single representative value for that interval. It includes also dealing with missing values, formatting, normalization and standardization. That makes it a technique for data smoothing.\n","\n","The **difference** between binning and bucketing is:\n","* In **bucketing**, we define the boundaries for the categories ourselves. They don't have to be the same size.\n","* In **binning**, we define the number of categories, and the code distributes them evenly across our dataset, depending on whether we set the binning criterion to the number of data in each bin, or to the value of the data.\n","\n","On a technical level, we to convert numeric values to categorical or to sample (quantise) numeric values, sometimes using distance (remember distance from one of the previous modules?) and sometimes using frequency. we can also reduce numeric values through sampling.\n","\n","Normalization and standardization are special cases of Discretization that we'll talk about in a bit. To learn more in depth about binning, go [here](https://towardsdatascience.com/data-preprocessing-with-python-pandas-part-5-binning-c5bd5fd1b950). To find out more about how to do this at a practical level, take a look at [this explanation](https://stackoverflow.com/questions/45273731/binning-column-with-python-pandas). "]},{"cell_type":"markdown","metadata":{"id":"DxmiSUvP_39G"},"source":["###Binning\n","It's time to look at incomeUSD. To remind yourself what the attribute looks like, use the describe() function to produce a 5-number summary:"]},{"cell_type":"code","metadata":{"id":"t8P0HrN3ANfx"},"source":["adult.incomeUSD.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ks-OBjCVDnsK"},"source":["We see that the income ranges from 20,002 to 199,965. How about making three categories: Low, medium, and high? If we are building 3 groups, we need 4 edges of intervals (**BINS**):\n","* small — (edge1, edge2)\n","* medium — (edge2, edge3)\n","* high — (edge3, edge4)\n","\n","We can use the linspace() function of the numpy package to calculate the 4 bins, equally distributed."]},{"cell_type":"code","metadata":{"id":"T64NDhl2ENpf"},"source":["bins = np.linspace(adult.incomeUSD.min(),adult.incomeUSD.max(),4)\n","bins"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Do8EHOXaHgzB"},"source":["Pandas has calculated the following cutoffs:\n","* Group 1: 20,002.00000 to 79,989.6666667\n","* Group 2: 79,989.6666667 to 139,977.3333333\n","* Group 3: 139,977.3333333 to 199,965.0000000\n","\n","Now we define the three labels (small, medium, high) for these three groups:"]},{"cell_type":"code","metadata":{"id":"BVpVkXFiHtCJ","executionInfo":{"status":"ok","timestamp":1622386102296,"user_tz":300,"elapsed":403,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":["labels = ['small', 'medium', 'high']"],"execution_count":261,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K7ca50_SH2oI"},"source":["We can use the cut() function to convert the numeric values of incomeUSD into the categorical values of small, medium, and high. We need to specify the bins and the labels. In addition, we set the parameter include_lowest to True in order to include also the minimum value. In order not to change our original dataset, we'll create an adult3 backup dataset:"]},{"cell_type":"code","metadata":{"id":"A2wS6iYGISuO","executionInfo":{"status":"ok","timestamp":1622386102301,"user_tz":300,"elapsed":404,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":["adult3=adult\n","adult3['bins'] = pd.cut(adult3['incomeUSD'], bins=bins, labels=labels, include_lowest=True)"],"execution_count":262,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XpftY9yYJKae"},"source":["This gives us the new 'bins' attribute at the end of our adult3 dataframe:"]},{"cell_type":"code","metadata":{"id":"QI3XNXFBKS8-"},"source":["adult3.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dGp6hc_2KeKe"},"source":["We can now use groupby() on this new 'bins' attribute to count the data in each of these categories:"]},{"cell_type":"code","metadata":{"id":"-PL_FSOjJWIk"},"source":["adult3.groupby(['bins']).count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u3cyUciNL7nn"},"source":["We can even plot them in a histogram:"]},{"cell_type":"code","metadata":{"id":"YgJt0fxbL-Us"},"source":["plt.hist(adult3['bins'], bins=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DbXIIiasMKPb"},"source":["Let's put our **CRITICAL THINKING** hats on: The cutoffs for the bins that we have calculated were:\n","* Group 1: 20,002.00000 to 79,989.6666667\n","* Group 2: 79,989.6666667 to 139,977.3333333\n","* Group 3: 139,977.3333333 to 199,965.0000000\n","\n","What we are seeing here is that a huge majority (26,397) of our datapoints falls into the \"small\" category, so below 79,989.67, whereas a joint 6,250 or so falls into the medium and high categories.\n","\n","Are these bins useful? Not really. The only thing they show us is that we have a whole lot of poor people in our dataset. Let's go back to the 5-number summary:"]},{"cell_type":"code","metadata":{"id":"7H-OPphgNjLf"},"source":["adult3.incomeUSD.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U7L7lvgmAP4O"},"source":["###Bucketing\n","We see that the income ranges from 20,002 to 199,965, with a median at $39,732. That's not even a fifth of the top salary! Such a low median means that 50% of the people in our dataset make less than 39,732 while the remaining 50% inhabit the range from 39,732 to 199,965. In fact, only 25% of people earn between 49,611 (Q3) and the top salary. Wouldn't it be interesting to see how many earners we have in the top range, between, say 150,000 and 199,000?\n","\n","That's where **BUCKETS** come in. \n","\n","With buckets, you set the interval cutoffs manually. So, how about we make 5 buckets: \n","* poor (20002 to the median of 39732)\n","* low-wage-earners (median of 39732 to Q3 aka 49631)\n","* middle-class (49631 to 79989 (which was the previous Group 2 cutoff))\n","* rich (79989 to 150000)\n","* super-rich (150000 to 199965)\n","\n","You're probably noticing that these buckets aren't the same size. With bucketing, that's not the point. We get to make the cutoffs.\n","\n","Now, let's code this.\n"]},{"cell_type":"code","metadata":{"id":"8M4MNCv_PwHC"},"source":["# Setting up our 5 buckets and their labels\n","buckets = [ 20002, 39732, 49631, 79989, 150000, 199965 ]\n","bucketlabels = ['poor', 'low-wage-earners', 'middle-class', 'rich', 'super-rich']\n","\n","# Using cut() to separate data into the buckets we have built\n","adult3['buckets'] = pd.cut(adult3['incomeUSD'] , bins=buckets, labels=bucketlabels, include_lowest=True)\n","\n","# Check if we have the new attribute 'buckets' at the end of our dataset\n","adult3.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IrrOUHnJRpzv"},"source":["# Checking the contents of our buckets\n","adult3.groupby(['buckets']).count()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C0gv6VPHRY2y"},"source":["# Plotting the distribution now\n","plt.hist(adult3['buckets'], bins=3)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YopuAWo7TPuw"},"source":["###**CRITICAL THINKING MOMENT**\n","As you can see, bucketing gives us much better information about our dataset, but we needed to use binning to point us in the right direction! Now, look at these three bucket cutoffs again:\n","\n","* poor (20002 to the median of 39732)\n","* rich (79989 to 150000)\n","* super-rich (150000 to 199965)\n","\n","We are seeing that the number of poor people is still vastly bigger than the numbers of rich and super-rich people. **AND** we see that most of the poverty is concentrated between 20002 and 39732--that's a range of less than 20000--whereas the range of rich to super-rich income is much larger, from 79989 to 199965--that's 120000!\n","\n","**WOW!**\n","\n","**DISCLAIMER**: Of course, we are evaluating the income from an American perspective, even though the numbers come from countries all over the world, and we are not looking at buying power by country, at all. For example, 39732 buys much more in a country like Germany than it does in the US. So, to get more balanced insights, we would have to do much more investigating!"]},{"cell_type":"markdown","metadata":{"id":"u6xIuxsFWacO"},"source":["##**4.3 Normalization and Standardization**\n","A third type of Discretization, which will become critical when we talk about Neural Networks, helps us compare the values inside numeric attributes in a dataset by putting them all on the same scale.\n","\n","Below, I will show you the most popular method for normalization, with the MixMaxScaler from the new-to-you scikit-learn package, which transforms values to a scale from 0 to 1. Then, I will show you two methods for standardization: A basic pandas method and the StandardScaler from the--you guessed it--scikit-learn package\n","\n","**CRITICAL THINKING MOMENT**\n","Which data would we want to compare? Typically the ones that can have meaning for one another, like the ones with the same units. So here, we will use age and educationyears to normalize because both are in the same unit: Years. \n","\n","First, we will pull out these two numeric attributes into their own dataframe. That will make our processing faster and easier"]},{"cell_type":"code","metadata":{"id":"zqV3dLuQhMXK"},"source":["adult_years = adult.iloc[:,[0,3]]\n","adult_years.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zrzeVmNyj7LH"},"source":["# Better safe than sorry! Let's check the dataypes.\n","adult_years.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dyk05eojo0V0"},"source":["###Normalization with scikit-learn\n","Scikit-learn is a scientific package that allows us to do biased estimating, based on how you tell the algorithm to calculate the normalization range. Some data scientists prefer the MixMax method over the pandas method because it gives more control over how to scale the attributes. The most popular approach is [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html), which scales and translates each attribute individually such that it falls between zero and one."]},{"cell_type":"code","metadata":{"id":"yN5my7TOqdkl"},"source":["# Normalization with MinMaxScaler\n","\n","from sklearn import preprocessing # the package\n","\n","adult_years1 = adult_years # backing up my dataframe--you never know!\n","x = adult_years1.values # pulling out just the array values\n","\n","# Now we are using the scaler\n","min_max_scaler = preprocessing.MinMaxScaler()\n","\n","# Check the link above for more explanation of this line\n","x_scaled = min_max_scaler.fit_transform(x) \n","\n","adult_years1 = pd.DataFrame(x_scaled)\n","adult_years1 # We should now be seeing all values between 0 and 1."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"89rOe8CZuc0u"},"source":["###Standardization with scikit-learn\n","Standardization is typically when values are organized around the mean and divided by the standard deviation. The fastest approach here is the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) from scikit-learn."]},{"cell_type":"code","metadata":{"id":"RXaxU2zLtaAm"},"source":["# Standardization with StandardScaler\n","\n","from sklearn.preprocessing import StandardScaler # the package\n","\n","adult_years2 = adult_years # backing up my dataframe--you never know!\n","scaler = StandardScaler() # setting an abbreviation to make things easier\n","\n","#adult_years2 = scaler.fit_transform(adult_years3.to_numpy()) # if you want to add an extra transformation to a numpy array\n","adult_years2 = scaler.fit_transform(adult_years3)\n","\n","adult_years2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uJZWOZTkfyma"},"source":["### Standardization with pandas\n","Using pandas for normalization is pretty straightforward: From each data value in an attribute, we subtract the mean and divide by the standard deviation. We do this row by row, so we specify axis=0. (axis=1 means column, remember?).\n","\n","Cycling through our data will require the lambda transformation we have seen in a previous module. To learn more about this lambda transformation, check [this link](https://www.analyticsvidhya.com/blog/2020/03/what-are-lambda-functions-in-python/)"]},{"cell_type":"code","metadata":{"id":"8DXl7KEygOh5"},"source":["adult_years3 = adult_years.apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n","adult_years3.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8HiBqlMHyAPf"},"source":["**OH! WAIT! WHAT!!!!???** \n","\n","Compare these values to the output for the StandardScaler--do you notice anything?\n"]},{"cell_type":"markdown","metadata":{"id":"eTOebMxpnRac"},"source":["### The Big Picture\n","If you don't want to build a separate dataframe, that's ok, but it takes up more processing power. You will have to work with iloc to tell Python which columns to transform (I am using adult3 below with the pandas transformation just because we did that last and because I don't want to break the original adult dataframe here)"]},{"cell_type":"code","metadata":{"id":"V8j5ifKNni2K"},"source":["adult3.iloc[:,[0,3]] = adult.iloc[:,[0,3]].apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n","adult3.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6svM7QYuEGSd"},"source":["##**Your Turn**\n","Now apply the MinMaxScaler to the age and educationyears attribute throughout the ENTIRE adult dataset (back it up as adult4 so you can always quickly replace it). For this, you will combine code from the MinMaxScaler normalization with the code directly above that applies the transformation to specific columns/ attributes/ dimensions/ features in the dataset."]},{"cell_type":"code","metadata":{"id":"uBa1eNNMEzsX"},"source":[""],"execution_count":null,"outputs":[]}]}